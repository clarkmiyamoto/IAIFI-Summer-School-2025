\section{Physics Informed ML}
The epistemology of ML \& Physics seems to be at odds with one another. If we start w/ Physics, we assert the model is true, and see what data comes out. In ML, we assert the data is true, and attempt to construct a (opaque) model).\\
\\
Perhaps to bring ML closer to Physics, we can use \textbf{inductive biases}. This is the "prior" of a model-- it gives a model a bias towards finding certain types of solutions. So perhaps we can use this to design models s.t. they're biased towards physical solutions. Here are a couple biases
\begin{itemize}
	\item Representation bias (data preparation):
	\item Hardware constraints (architecture design): Put symmetries in the NN (ie. geometric deep learning, equivariance)
	\item Software constraints (loss function design): Soft impose conservation laws by adding penalties in loss function.
	\item Implicit bias (data augmentation): Construct training examples s.t. model learns specific features of the problem.
\end{itemize}
\subsection{Graph Structured Data}
Consider a data which is encoded into a graph structure (i.e. 2D lattice)



\section{Physics Inspired ML}